{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T11:02:06.266210Z",
     "start_time": "2023-02-23T11:02:00.126986Z"
    },
    "id": "zriTdjauH8iQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/16971921/.conda/envs/py38_venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-23 14:02:00.466774: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQiRPWWHlSgv"
   },
   "source": [
    "### Using pre-trained transformers (seminar is worth 2 points)\n",
    "_for fun and profit_\n",
    "\n",
    "There are many toolkits that let you access pre-trained transformer models, but the most powerful and convenient by far is [`huggingface/transformers`](https://github.com/huggingface/transformers). In this week's practice, you'll learn how to download, apply and modify pre-trained transformers for a range of tasks. Buckle up, we're going in!\n",
    "\n",
    "\n",
    "__Pipelines:__ if all you want is to apply a pre-trained model, you can do that in one line of code using pipeline. Huggingface/transformers has a selection of pre-configured pipelines for masked language modelling, sentiment classification, question aswering, etc. ([see full list here](https://huggingface.co/transformers/main_classes/pipelines.html))\n",
    "\n",
    "A typical pipeline includes:\n",
    "* pre-processing, e.g. tokenization, subword segmentation\n",
    "* a backbone model, e.g. bert finetuned for classification\n",
    "* output post-processing\n",
    "\n",
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T11:02:09.419333Z",
     "start_time": "2023-02-23T11:02:06.268616Z"
    },
    "id": "rP1KFtvLlJHR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998860359191895}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "print(classifier(\"BERT is amazing!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T11:02:09.579276Z",
     "start_time": "2023-02-23T11:02:09.421599Z"
    },
    "id": "nYUNuyXMn5l9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "data = {\n",
    "    'arryn': 'As High as Honor.',\n",
    "    'baratheon': 'Ours is the fury.',\n",
    "    'stark': 'Winter is coming.',\n",
    "    'tyrell': 'Growing strong.'\n",
    "}\n",
    "\n",
    "# YOUR CODE: predict sentiment for each noble house and create outputs dict\n",
    "# <...>\n",
    "# outputs = <YOUR CODE: dict (house name) : True if positive, False if negative>\n",
    "outputs = {k:   True  if classifier.predict(data[k])[0]['label']=='POSITIVE'\n",
    "           else False if classifier.predict(data[k])[0]['label']=='NEGATIVE'\n",
    "           else None\n",
    "           for k in data.keys()}\n",
    "\n",
    "assert sum(outputs.values()) == 3 and outputs[base64.decodebytes(b'YmFyYXRoZW9u\\n').decode()] == False\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRDhIH-XpSNo"
   },
   "source": [
    "You can also access vanilla Masked Language Model that was trained to predict masked words. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T11:02:11.570151Z",
     "start_time": "2023-02-23T11:02:09.582920Z"
    },
    "id": "pa-8noIllRbZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P=0.99719 donald trump is the president of the united states.\n",
      "P=0.00024 donald duck is the president of the united states.\n",
      "P=0.00022 donald ross is the president of the united states.\n",
      "P=0.00020 donald johnson is the president of the united states.\n",
      "P=0.00018 donald wilson is the president of the united states.\n"
     ]
    }
   ],
   "source": [
    "mlm_model = pipeline('fill-mask', model=\"bert-base-uncased\")\n",
    "MASK = mlm_model.tokenizer.mask_token\n",
    "\n",
    "for hypo in mlm_model(f\"Donald {MASK} is the president of the united states.\"):\n",
    "    print(f\"P={hypo['score']:.5f}\", hypo['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T11:02:11.673241Z",
     "start_time": "2023-02-23T11:02:11.572650Z"
    },
    "id": "9NxeG1Y5pwX1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.88453 seq: the soviet union was founded in.\n"
     ]
    }
   ],
   "source": [
    "# Your turn: use bert to recall what year was the Soviet Union founded in\n",
    "mask_predict = mlm_model(f'the Soviet Union was founded in {MASK}')\n",
    "print(f\"score: {mask_predict[0]['score']:.5f}\", 'seq:', mask_predict[0]['sequence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJxRFzCSq903"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Huggingface offers hundreds of pre-trained models that specialize on different tasks. You can quickly find the model you need using [this list](https://huggingface.co/models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T11:02:18.952841Z",
     "start_time": "2023-02-23T11:02:11.675629Z"
    },
    "id": "HRux8Qp2hkXr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Almost two-thirds of the 1.5 million people who viewed this liveblog had Googled to discover\n",
    " the latest on the Rosetta mission. They were treated to this detailed account by the Guardian’s science editor,\n",
    " Ian Sample, and astronomy writer Stuart Clark of the moment scientists landed a robotic spacecraft on a comet \n",
    " for the first time in history, and the delirious reaction it provoked at their headquarters in Germany.\n",
    "  “We are there. We are sitting on the surface. Philae is talking to us,” said one scientist.\n",
    "\"\"\"\n",
    "\n",
    "# Task: create a pipeline for named entity recognition, use task name 'ner' and search for the right model in the list\n",
    "ner_model = pipeline('ner', model='Jean-Baptiste/roberta-large-ner-english')\n",
    "\n",
    "named_entities = ner_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T11:02:18.961923Z",
     "start_time": "2023-02-23T11:02:18.955460Z"
    },
    "id": "hf57MRzSiSON"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: [{'entity': 'MISC', 'score': 0.93693805, 'index': 18, 'word': 'ĠGo', 'start': 73, 'end': 75}, {'entity': 'MISC', 'score': 0.8974456, 'index': 19, 'word': 'og', 'start': 75, 'end': 77}, {'entity': 'MISC', 'score': 0.651527, 'index': 20, 'word': 'led', 'start': 77, 'end': 80}, {'entity': 'MISC', 'score': 0.9767904, 'index': 28, 'word': 'ĠRos', 'start': 112, 'end': 115}, {'entity': 'MISC', 'score': 0.98765314, 'index': 29, 'word': 'etta', 'start': 115, 'end': 119}, {'entity': 'ORG', 'score': 0.9986755, 'index': 41, 'word': 'ĠGuardian', 'start': 179, 'end': 187}, {'entity': 'PER', 'score': 0.99974495, 'index': 49, 'word': 'ĠIan', 'start': 207, 'end': 210}, {'entity': 'PER', 'score': 0.9996376, 'index': 50, 'word': 'ĠSample', 'start': 211, 'end': 217}, {'entity': 'PER', 'score': 0.99972564, 'index': 55, 'word': 'ĠStuart', 'start': 240, 'end': 246}, {'entity': 'PER', 'score': 0.99972826, 'index': 56, 'word': 'ĠClark', 'start': 247, 'end': 252}, {'entity': 'LOC', 'score': 0.99986243, 'index': 89, 'word': 'ĠGermany', 'start': 414, 'end': 421}, {'entity': 'MISC', 'score': 0.5681052, 'index': 106, 'word': 'ĠPhil', 'start': 471, 'end': 475}, {'entity': 'MISC', 'score': 0.8795991, 'index': 107, 'word': 'ae', 'start': 475, 'end': 477}]\n",
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "print('OUTPUT:', named_entities)\n",
    "word_to_entity = {item['word'].replace('Ġ',''): item['entity'] for item in named_entities}\n",
    "assert 'org' in word_to_entity.get('Guardian').lower() and 'per' in word_to_entity.get('Stuart').lower()\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULMownz6sP9n"
   },
   "source": [
    "### The building blocks of a pipeline\n",
    "\n",
    "Huggingface also allows you to access its pipelines on a lower level. There are two main abstractions for you:\n",
    "* `Tokenizer` - converts from strings to token ids and back\n",
    "* `Model` - a pytorch `nn.Module` with pre-trained weights\n",
    "\n",
    "You can use such models as part of your regular pytorch code: insert is as a layer in your model, apply it to a batch of data, backpropagate, optimize, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T11:02:20.377211Z",
     "start_time": "2023-02-23T11:02:18.964889Z"
    },
    "id": "KMJbV0QVsO0Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T11:02:20.392743Z",
     "start_time": "2023-02-23T11:02:20.378867Z"
    },
    "id": "ZgSPHKPRxG6U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[ 101, 5355, 1010, 1045, 2572, 2115, 2269, 1012,  102,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 101, 2166, 2003, 2054, 6433, 2043, 2017, 1005, 2128, 5697, 2437, 2060,\n",
      "         3488, 1012,  102]])\n",
      "token_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "\n",
      "Detokenized:\n",
      "[CLS] luke, i am your father. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] life is what happens when you're busy making other plans. [SEP]\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    \"Luke, I am your father.\",\n",
    "    \"Life is what happens when you're busy making other plans.\",\n",
    "    ]\n",
    "\n",
    "# tokenize a batch of inputs. \"pt\" means [p]y[t]orch tensors\n",
    "tokens_info = tokenizer(lines, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "for key in tokens_info:\n",
    "    print(key, tokens_info[key])\n",
    "\n",
    "print(\"\\nDetokenized:\")\n",
    "for i in range(2):\n",
    "    print(tokenizer.decode(tokens_info['input_ids'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-23T11:02:20.535011Z",
     "start_time": "2023-02-23T11:02:20.399492Z"
    },
    "id": "MJkbHxERyfL4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8854, -0.4722, -0.9392,  ..., -0.8081, -0.6955,  0.8748],\n",
      "        [-0.9297, -0.5161, -0.9334,  ..., -0.9017, -0.7492,  0.9201]])\n"
     ]
    }
   ],
   "source": [
    "# You can now apply the model to get embeddings\n",
    "with torch.no_grad():\n",
    "    out = model(**tokens_info)\n",
    "\n",
    "print(out['pooler_output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHEC6o7uAfgQ"
   },
   "source": [
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "__Bonus demo:__ transformer language models. \n",
    "\n",
    "`/* No points awarded for this task, but its really cool, we promise :) */`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T14:16:58.315168Z",
     "start_time": "2023-02-21T14:16:51.575767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Fermi paradox "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').train(False).to(device)\n",
    "\n",
    "text = \"The Fermi paradox \"\n",
    "tokens = tokenizer.encode(text)\n",
    "num_steps = 1024 - len(tokens) + 1\n",
    "line_length, max_length = 0, 70\n",
    "\n",
    "print(end=tokenizer.decode(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T14:17:30.962257Z",
     "start_time": "2023-02-21T14:17:30.910078Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[383, 376, 7780, 72, 22226, 220]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T14:18:59.574161Z",
     "start_time": "2023-02-21T14:18:59.558776Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1019"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T14:20:00.539949Z",
     "start_time": "2023-02-21T14:19:58.436896Z"
    },
    "id": "vWCajBGcAern"
   },
   "outputs": [],
   "source": [
    "for i in range(num_steps):\n",
    "    with torch.no_grad():\n",
    "        # logits (prediction_scores) of shape (batch_size, sequence_length, config.vocab_size)\n",
    "        logits = model(torch.as_tensor([tokens], device=device))[0] \n",
    "    p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\n",
    "\n",
    "    next_token_index = p_next.argmax() #<YOUR CODE: REPLACE THIS LINE>\n",
    "    # YOUR TASK: change the code so that it performs nucleus sampling\n",
    "\n",
    "    # tokens.append(int(next_token_index))\n",
    "    # print(end=tokenizer.decode(tokens[-1]))\n",
    "    # line_length += len(tokenizer.decode(tokens[-1]))\n",
    "    # if line_length >= max_length:\n",
    "    #     line_length = 0\n",
    "    #     print()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T14:38:16.407312Z",
     "start_time": "2023-02-21T14:38:16.376963Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.as_tensor([tokens], device=device).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T14:20:16.108125Z",
     "start_time": "2023-02-21T14:20:16.091496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 50257])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T14:26:44.011056Z",
     "start_time": "2023-02-21T14:26:43.998657Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits[0, -1, :].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T14:42:55.787028Z",
     "start_time": "2023-02-21T14:42:55.737762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50257])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(logits[0, -1, :], dim=-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T14:44:01.647745Z",
     "start_time": "2023-02-21T14:44:01.625758Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.7383505e-05, 4.1066254e-05, 2.1002043e-06, ..., 5.9595684e-09,\n",
       "       7.3726596e-08, 2.6800011e-05], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-21T14:43:55.260341Z",
     "start_time": "2023-02-21T14:43:55.230586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1849,  5523,  2602, ..., 30161, 45496, 32207])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(-p_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/aulasau/ysda_nlp_course/blob/7fba752598f280b936323f85e3ec711efa44197e/week_5/week_5_seminar.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Vij7Gc1wOaq"
   },
   "source": [
    "Transformers knowledge hub: https://huggingface.co/transformers/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "seminar.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
