{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFYpvF5NYJDE"
   },
   "source": [
    "# Text to Speech\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yNfJOv-cYJDO"
   },
   "source": [
    "![this should be a picture](https://i.imgur.com/WQXFdci.png)\n",
    "\n",
    "Last week was all about sound processing: you learned about audio files, spectrograms and even trained a simple speech classifier on top of that. This time we'll do the same, but the other way around: the should take text as an input and generate sound from that. Jump in! it's gonna be fun :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JKHAeXy-YJDW"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wc4bm4oxYJDu"
   },
   "source": [
    "## Data\n",
    "We took LJSpeech and carefully extracted features (Mels-specs and phone alignment) so you don't have to.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdrYtiXRYJDy"
   },
   "outputs": [],
   "source": [
    "!pip install librosa tensorflow-gpu==2.1.0rc0\n",
    "!wget https://www.dropbox.com/s/fmvi648spv8xjxd/cmudict.dict?dl=1 -O cmudict.dict\n",
    "!wget https://www.dropbox.com/s/ihhs20xws1jstvu/dataset-aligned.tar?dl=1 -O dataset-aligned.tar\n",
    "!wget https://www.dropbox.com/s/zvyqz4ovx84gaw1/waveglow_256channels.pt?dl=1 -O waveglow_256channels.pt\n",
    "!tar -xf dataset-aligned.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w1Qjji84YJEJ"
   },
   "outputs": [],
   "source": [
    "all_rows = [\n",
    "    json.load(open(fname, encoding='utf8'))\n",
    "    for fname in glob.glob('dataset-aligned/*.json')\n",
    "]\n",
    "assert len(all_rows) == 13100\n",
    "\n",
    "id2utt = {row['ID']: row['utterance'] for row in all_rows if 'utterance' in row}\n",
    "\n",
    "all_ids = sorted(id2utt.keys())\n",
    "assert len(id2utt) == 13071\n",
    "\n",
    "NMels = 80\n",
    "\n",
    "id2mel = {\n",
    "    ID: np.load('dataset-aligned/{}.mel.npy'.format(ID))\n",
    "    for ID in id2utt\n",
    "}\n",
    "for mels in id2mel.values():\n",
    "    assert mels.shape[0] == NMels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_FL2UJt-YJEd"
   },
   "source": [
    "## What is in an utterance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mA2B5p6aYJEg"
   },
   "outputs": [],
   "source": [
    "id2utt[all_ids[1231]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8NSGncHDYJEp"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ms2frames(ms, SR=22050, hop_length = 256):\n",
    "    return int(ms / 1000 * SR / hop_length)\n",
    "\n",
    "def show_utt(utt, mels, contour=False):\n",
    "    print(' '.join(w['text'] for w in utt['words']).replace('yandexttsspecialpauseword', 'PAUSE'))\n",
    "    plt.figure(figsize=[mels.shape[1] / 10, 5])\n",
    "    plt.imshow(mels[::-1], aspect='auto')\n",
    "    if contour:\n",
    "        plt.contour(mels[::-1], levels = 5,colors='w')\n",
    "    for word in utt['words']:\n",
    "        onset = ms2frames(word['phones'][0]['onset'])\n",
    "        plt.text(onset + 1, -20, word['text'].replace('yandexttsspecialpauseword', 'PAUSE'))\n",
    "        plt.plot([onset] * 2, [-20, 0], 'k')\n",
    "        for phone in word['phones']:\n",
    "            onset = ms2frames(phone['onset'])\n",
    "            plt.text(onset + 1, -10, phone['phone'])\n",
    "            plt.plot([onset] * 2, [-10, 0], 'k')\n",
    "            plt.plot([onset] * 2, [80, 0], 'w')\n",
    "    plt.show()\n",
    "    \n",
    "ID = all_ids[45]\n",
    "show_utt(id2utt[ID], id2mel[ID])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RxTdmwH7YJE1"
   },
   "source": [
    "## Listen\n",
    "We'll use a pre-trained Waveglow vocoder:\n",
    "https://github.com/NVIDIA/waveglow\n",
    "\n",
    "It's written in PyTorch. If you need to install it locally, [here](https://pytorch.org/)'s how you do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjlLJI34YJE4"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/NVIDIA/tacotron2.git\n",
    "!cd tacotron2 && git submodule update --init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xMYT_5VjYJFA"
   },
   "outputs": [],
   "source": [
    "## Import and load model\n",
    "import torch\n",
    "import sys\n",
    "torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if torch_device == 'cpu':\n",
    "  torch.cuda.FloatTensor = torch.FloatTensor  # dirty trick to run old WaveGlow\n",
    "\n",
    "WGP = 'tacotron2/waveglow/'\n",
    "if WGP not in sys.path:\n",
    "    sys.path.append(WGP)\n",
    "\n",
    "waveglow_path = 'waveglow_256channels.pt'\n",
    "waveglow = torch.load(waveglow_path, map_location=torch_device)['model']\n",
    "waveglow.to(torch_device).train(False)\n",
    "if torch_device == 'cuda':\n",
    "    waveglow = waveglow.half()\n",
    "    for k in waveglow.convinv:\n",
    "        k.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGTgWlEvYJFK"
   },
   "outputs": [],
   "source": [
    "## Synthesize and listen\n",
    "## Don't mind the warnings, it's an old model checkpoint\n",
    "import IPython.display as ipd\n",
    "def synthesize(mels, SR=22050):\n",
    "    with torch.no_grad():\n",
    "        torch_batch = torch.as_tensor(\n",
    "            mels[None, :, :], device=torch_device, \n",
    "            dtype=torch.float16 if torch_device=='cuda' else torch.float32)\n",
    "        audio = waveglow.infer(torch_batch, sigma=1)\n",
    "    ipd.display(ipd.Audio(audio[0].data.cpu().numpy(), rate=SR, autoplay=True))\n",
    "\n",
    "\n",
    "synthesize(id2mel[all_ids[32]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m-9oOcZOYJFS"
   },
   "source": [
    "## Build phoneme dictionary (the usual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FtOT0DovYJFV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "phone_counts = Counter(\n",
    "    phone['phone'] \n",
    "    for utt in id2utt.values() \n",
    "    for word in utt['words'] \n",
    "    for phone in word['phones']\n",
    ")\n",
    "PAD = '_PAD'\n",
    "\n",
    "# Task: create phoneme vocabulary that maps phonemes to ids\n",
    "# Note: your words should be sorted by python string order\n",
    "all_phones = <YOUR CODE HERE>\n",
    "phone2idx = <YOUR CODE HERE>\n",
    "assert len(all_phones) == 55\n",
    "assert all_phones[-1] == PAD\n",
    "assert phone2idx[all_phones[0]] == 0\n",
    "assert phone2idx[all_phones[13]] == 13\n",
    "assert phone2idx[all_phones[-1]] == 54\n",
    "\n",
    "print('All good!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wo-M1HgYJFe"
   },
   "source": [
    "## Let's look at data:\n",
    "### Phone durations histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJUh5bz1YJFg"
   },
   "outputs": [],
   "source": [
    "all_durations = np.array([phone['duration'] for utt in id2utt.values() for word in utt['words'] for phone in word['phones']])\n",
    "plt.hist(all_durations, bins=100, log=True)\n",
    "plt.show()\n",
    "print('mean={}'.format(np.mean(all_durations)))\n",
    "print('median={}'.format(np.median(all_durations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EYItFRetYJFn"
   },
   "source": [
    "### Sentence lengths in frames and in phones:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yxojkugPYJFp"
   },
   "outputs": [],
   "source": [
    "#sent lengths in frames and in phones:\n",
    "\n",
    "phone_counts = [sum(len(word['phones']) for word in utt['words']) for utt in id2utt.values()]\n",
    "frame_counts = [mels.shape[1] for mels in id2mel.values()]\n",
    "\n",
    "plt.figure(figsize=[10,10])\n",
    "plt.subplot(2,2,1)\n",
    "plt.hist(phone_counts, bins=30)\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(sorted(phone_counts))\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.hist(frame_counts, bins=30)\n",
    "plt.subplot(2,2,4)\n",
    "plt.plot(sorted(frame_counts))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wfXdL11YJFx"
   },
   "source": [
    "### Melspec distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGJHn10yYJFz"
   },
   "outputs": [],
   "source": [
    "some_mels = np.concatenate([id2mel[ID] for ID in random.sample(all_ids, 100)], axis=1)\n",
    "\n",
    "mel_means = np.mean(some_mels, axis=1)\n",
    "mel_stds = np.std(some_mels, axis=1)\n",
    "\n",
    "plt.plot(mel_means, label='mean')\n",
    "plt.plot(mel_means + mel_stds, label = 'mean + std')\n",
    "plt.plot(mel_means - mel_stds, label = 'mean - std')\n",
    "plt.plot(mel_stds, label = 'std')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mel_corr = np.corrcoef(some_mels)\n",
    "plt.imshow(mel_corr)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iDtAyg9mYJF8"
   },
   "source": [
    "## Generating batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJ43BUQxYJF-"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "TtsBatch = namedtuple('TtsBatch', [\n",
    "    'phone_idxs', # (B x Lenc), int\n",
    "    'phone_durs', # (B x Lenc), float\n",
    "    'alignment',  # (B x Ldec), int\n",
    "    'mels',       # (B x Ldec x Nmels), float\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tJgD8gtyYJGD"
   },
   "outputs": [],
   "source": [
    "def gen_batch(ids):\n",
    "    \"\"\" Assemble training batch from sample indices \"\"\"\n",
    "    utts = [id2utt[ID] for ID in ids]\n",
    "    phone_seqs = [[phone2idx[phone['phone']] for word in utt['words'] for phone in word['phones']] for utt in utts]\n",
    "    phone_durs = [[phone['duration'] for word in utt['words'] for phone in word['phones']] for utt in utts]\n",
    "    phone_seq_mat = np.full([len(ids), max(map(len, phone_seqs))], phone2idx[PAD], dtype='int32')\n",
    "    phone_dur_mat = np.ones([len(ids), max(map(len, phone_seqs))], dtype='float32')\n",
    "    for i, (idxs, durs) in enumerate(zip(phone_seqs, phone_durs)):\n",
    "        phone_seq_mat[i, :len(idxs)] = idxs\n",
    "        phone_dur_mat[i, :len(idxs)] = durs\n",
    "        \n",
    "        \n",
    "    mels = [id2mel[ID] for ID in ids]\n",
    "    mel_lengths = np.array([mel.shape[1] for mel in mels], dtype='int32')\n",
    "    mel_mat = np.full([len(ids), max(mel_lengths), NMels], -1, dtype='float32')\n",
    "    mel_aligns = np.full([len(ids), max(mel_lengths)], -1, dtype='int32')\n",
    "    for i, mel in enumerate(mels):\n",
    "        mel_mat[i,  :mel_lengths[i]] = mel.T\n",
    "        for j, phone in enumerate(phone for word in utts[i]['words'] for phone in word['phones']):\n",
    "            start = ms2frames(phone['onset'])\n",
    "            finish = ms2frames(phone['onset'] + phone['duration'])\n",
    "            mel_aligns[i, start:finish] = j\n",
    "    return TtsBatch(\n",
    "        phone_idxs=phone_seq_mat,\n",
    "        phone_durs=phone_dur_mat,\n",
    "        alignment=mel_aligns,\n",
    "        mels=mel_mat\n",
    "    )\n",
    "    \n",
    "    \n",
    "gen_batch(all_ids[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EjAJ5ltbYJGM"
   },
   "outputs": [],
   "source": [
    "valid_ids = all_ids[:100]\n",
    "train_ids = all_ids[100:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtCFTlgFYJGS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "is_gpu_available = any(device.device_type == 'GPU' for device in device_lib.list_local_devices())\n",
    "device = '/device:GPU:0' if is_gpu_available else '/device:CPU:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model & training __(3 points)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M7jvKjCFYJGc"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2'), \"Current tf version: {}; required: 2.0.*\".format(tf.__version__)\n",
    "L = tf.keras.layers\n",
    "keras = tf.keras\n",
    "\n",
    "class Model(L.Layer):\n",
    "    def __init__(\n",
    "        self, \n",
    "        phone_count=len(all_phones), \n",
    "        emb_size=128, \n",
    "        enc_hid_size=128, \n",
    "        dec_hid_size=128,\n",
    "    ):\n",
    "        super().__init__() # initialize base class to track sub-layers, trainable variables, etc.\n",
    "\n",
    "        # Phoneme embedding\n",
    "        self.emb = L.Embedding(phone_count, emb_size)\n",
    "        # Encoder cell\n",
    "        self.encoder = L.GRU(enc_hid_size, return_sequences=True)\n",
    "        # Duration predictor\n",
    "        self.dur_linear = L.Dense(\n",
    "            1, bias_initializer=keras.initializers.constant(np.mean(all_durations))\n",
    "            # Karpathy's trick: http://karpathy.github.io/2019/04/25/recipe/\n",
    "\n",
    "        )\n",
    "        # Decoder cell\n",
    "        self.decoder = L.GRU(dec_hid_size, return_sequences=True, return_state=True)\n",
    "\n",
    "        # Melspec predictor\n",
    "        self.mel_projection = L.Dense(\n",
    "            NMels, bias_initializer=keras.initializers.constant(mel_means)\n",
    "        ) \n",
    "        \n",
    "        self.zeroth_mel = tf.Variable(np.random.normal(size=[NMels]), dtype='float32', name='zero_mel')\n",
    "\n",
    "    def encode(self, \n",
    "               phone_idxs, # B x Lenc\n",
    "               train=False):\n",
    "        # Encode phonemes and predict durations from hidden state.\n",
    "        # You should use: emb, encoder, dur_linear\n",
    "        <YOUR_CODE>\n",
    "        return (\n",
    "            durations , # B x Lenc x 1\n",
    "            hid_state   # B x Lenc x Henc\n",
    "        )\n",
    "    \n",
    "\n",
    "    def decode(self, \n",
    "               encoded,    # B x Lenc x Henc\n",
    "               alignments, # B x Ldec\n",
    "               prev_mels,  # B x Ldec x NMels\n",
    "               prev_states, # None or list of RNN cell(s) states\n",
    "               train=False):\n",
    "        encoded_upsampled = tf.gather_nd(encoded, tf.maximum(0, alignments[:,:,None]), batch_dims=1)\n",
    "        X = tf.concat([encoded_upsampled, prev_mels], axis=2)\n",
    "\n",
    "        # Run decoder recurrent network over X. Start from prev_states\n",
    "        # After that you can predict next mels using mel_projection\n",
    "        <YOUR_CODE>\n",
    "    \n",
    "        return (\n",
    "            mels,      # B x Ldec X NMels\n",
    "            new_states # list of states\n",
    "        )\n",
    "    \n",
    "    def forward_train(self, batch, train=False):\n",
    "        # This runs the model in train mode for calculating loss + optionally gradients,\n",
    "        # using teacher forcing on mels\n",
    "        \n",
    "        # Prepare\n",
    "        zeroth_mel = tf.tile(self.zeroth_mel[None, None, :], [batch.mels.shape[0], 1, 1])\n",
    "        prev_mels = tf.concat([zeroth_mel, batch.mels[:,:-1]], axis=1)\n",
    "        prev_states = [None]\n",
    "        \n",
    "        #Run encoder\n",
    "        durs, encoded = self.encode(batch.phone_idxs, train=train)\n",
    "        #Run decoder\n",
    "        mel_preds, _ = self.decode(encoded, batch.alignment, prev_mels, prev_states, train=train)\n",
    "        \n",
    "        return durs, mel_preds\n",
    "    \n",
    "    def forward_inference(self, \n",
    "                          phone_idx # Is flattened, doesn't work with batches\n",
    "                          ):\n",
    "        # This runs the model in inference mode, using its own predicted durations and mels from prev. step\n",
    "        prev_mels = self.zeroth_mel[None, None, :]\n",
    "        prev_states = [None]\n",
    "        # Run encoder\n",
    "        durs, encoded = self.encode(phone_idx.reshape([1,-1]))\n",
    "        # Convert frame durations to alignments\n",
    "        frame_durs = list(map(ms2frames, durs[0]))\n",
    "        frame_durs = np.maximum(1, np.array(frame_durs))\n",
    "        full_alignment = np.array(sum([[i] * frames for i, frames in enumerate(frame_durs)], []))\n",
    "        \n",
    "        # Run decoder, one step at a time, reusing previous states and mels\n",
    "        result = []\n",
    "        for frame_alignment in full_alignment:\n",
    "            mel_preds, states = self.decode(encoded, np.full([1,1], frame_alignment), prev_mels, prev_states)\n",
    "            result.append(mel_preds)\n",
    "            prev_mels, prev_states = mel_preds, states\n",
    "            \n",
    "        # Collect mels\n",
    "        mels = tf.concat(result, axis=1)\n",
    "        return mels\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVC8GAqfYJGh"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1337)\n",
    "np.random.seed(1337)\n",
    "\n",
    "model = Model()\n",
    "batch = gen_batch(all_ids[78:80])\n",
    "dur_pred, mel_pred = model.forward_train(batch)\n",
    "assert dur_pred.shape == (2, 75, 1) \n",
    "assert mel_pred.shape ==  (2, 583, 80)\n",
    "assert np.allclose(dur_pred[:,20,0].numpy(), [88.01232, 88.02252], atol=0.1, rtol=0.1)\n",
    "assert np.allclose(mel_pred[0, 100, :5].numpy(), [-6.5848618, -6.194147 , -6.006989 , -4.6337852, -3.1684837], atol=0.1, rtol=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6D9QqFRZYJGo"
   },
   "outputs": [],
   "source": [
    "mels = model.forward_inference(batch.phone_idxs)\n",
    "assert mels.shape == (1, 1050, 80)\n",
    "print(mels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zrz81YopYJGx"
   },
   "outputs": [],
   "source": [
    "[(v.name, v.shape) for v in model.trainable_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o3QpMIWBYJG2"
   },
   "outputs": [],
   "source": [
    "def dur_loss(\n",
    "    dur_true,  # B x Lenc\n",
    "    dur_pred,  # B x Lenc x 1\n",
    "    phone_idxs # phoneme indices B x Lenc\n",
    "    ):\n",
    "    mask = tf.cast(phone_idxs != phone2idx[PAD], dtype='float32') # B x Lenc\n",
    "\n",
    "    # Evaluate Mean Absolute Error (L1) between predicted and true durations\n",
    "    # Note: your should average loss only over cells where mask equals True\n",
    "    <YOUR CODE HERE>\n",
    "    \n",
    "    # Warning: mind the shapes! they are a bit nonintuitive\n",
    "\n",
    "    return <YOUR LOSS>\n",
    "\n",
    "\n",
    "def mel_loss(mel_true,    # B x Ldec x Nmels\n",
    "             mel_pred,    # B x Ldec x Nmels\n",
    "             alignments): # B x Ldec\n",
    "    mask = tf.cast(alignments >= 0, dtype='float32')\n",
    "\n",
    "    # Compute Mean Squared Error (L2) between predicted and true mel spectres\n",
    "    # Note: same as before, average over all active (mask) cells AND over mel channels\n",
    "\n",
    "    <YOUR CODE HERE>\n",
    "    return <YOUR LOSS>\n",
    "\n",
    "batch = gen_batch([all_ids[78]])\n",
    "dur_pred, mel_pred = model.forward_train(batch)\n",
    "\n",
    "loss_dur = dur_loss(batch.phone_durs, dur_pred, batch.phone_idxs)\n",
    "assert loss_dur.shape == ()\n",
    "assert np.allclose(loss_dur.numpy() / 30, 1.3000526, rtol=1e-2, atol=1e-2)\n",
    "\n",
    "loss_mel = mel_loss(batch.mels, mel_pred, batch.alignment)\n",
    "assert loss_mel.shape == ()\n",
    "assert np.allclose(loss_mel.numpy(), 3.4176075, rtol=1e-2, atol=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dtTfgAHNYJG8"
   },
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    metrics = {'train_loss': [], 'valid_loss': [] }\n",
    "    model = Model()\n",
    "    opt = keras.optimizers.Adam(1e-3)\n",
    "    batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kLkPddnEYJHD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "valid_batch = gen_batch(valid_ids)\n",
    "with tf.device(device):\n",
    "    while True:\n",
    "        print(end='.')\n",
    "        batch = gen_batch(random.sample(train_ids, batch_size))\n",
    "        step = len(metrics['train_loss']) + 1\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            dur_pred, mel_pred = model.forward_train(batch)\n",
    "            loss_dur = dur_loss(batch.phone_durs, dur_pred, batch.phone_idxs)\n",
    "            loss_mel = mel_loss(batch.mels, mel_pred, batch.alignment)\n",
    "            loss_t = loss_dur / 30 + loss_mel\n",
    "\n",
    "        grads = tape.gradient(loss_t, model.trainable_variables)\n",
    "        opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        metrics['train_loss'].append((step, loss_t.numpy(), loss_dur.numpy(), loss_mel.numpy()))\n",
    "        if step % 20 == 0:\n",
    "            dur_pred, mel_pred = model.forward_train(valid_batch)\n",
    "            loss_dur = dur_loss(valid_batch.phone_durs, dur_pred, valid_batch.phone_idxs)\n",
    "            loss_mel = mel_loss(valid_batch.mels, mel_pred, valid_batch.alignment)\n",
    "            loss_v = loss_dur / 30 + loss_mel\n",
    "            metrics['valid_loss'].append((step, loss_v.numpy(), loss_dur.numpy(), loss_mel.numpy()))\n",
    "\n",
    "            ipd.clear_output(True)\n",
    "            plt.figure(figsize=(12,4))\n",
    "            for i, (name, history) in enumerate(sorted(metrics.items())):\n",
    "                plt.subplot(1, len(metrics), i + 1)\n",
    "                plt.title(name)\n",
    "                history = np.array(history, dtype='float32').T\n",
    "                plt.plot(history[0], history[1:].T)\n",
    "                plt.grid()\n",
    "                plt.legend(['total', 'duration', 'mel_pred'])\n",
    "            plt.show()\n",
    "            synthesized = mel_pred[0, :id2mel[valid_ids[0]].shape[1]].numpy().T\n",
    "            show_utt(id2utt[valid_ids[0]], id2mel[valid_ids[0]])\n",
    "            show_utt(id2utt[valid_ids[0]], synthesized)\n",
    "            print(\"Mean loss=%.3f, valid=%.3f\" % (np.mean(metrics['train_loss'][-10:], axis=0)[1], \n",
    "                                                  metrics['valid_loss'][-1][1]), flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jr6prCL-YJHH"
   },
   "outputs": [],
   "source": [
    "mels = model.forward_inference(valid_batch.phone_idxs[8][:36])\n",
    "mels = mels.numpy()\n",
    "synthesize(mels[0].T)\n",
    "plt.imshow(mels[0, ::-1].T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KREMCjlJYJHQ"
   },
   "source": [
    "For Text2Speech we need a dictionary\n",
    "\n",
    "https://github.com/cmusphinx/cmudict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "suVKDYl1YJHS",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load dictionary\n",
    "\n",
    "import collections\n",
    "import re\n",
    "en_g2p_dict = collections.defaultdict(list)\n",
    "phone_remapping = {\n",
    "    'AA0': 'AA1',\n",
    "    'AA2': 'AA1',\n",
    "    'AE2': 'AE1',\n",
    "    'AH2': 'AH1',\n",
    "    'AO2': 'AO1',\n",
    "    'AW2': 'AW1',\n",
    "    'AY2': 'AY1',\n",
    "    'EH2': 'EH1',\n",
    "    'ER0': 'EH1',\n",
    "    'ER1': 'EH1',\n",
    "    'ER2': 'EH1',\n",
    "    'EY2': 'EY1',\n",
    "    'IH2': 'IH1',\n",
    "    'IY2': 'IY1',\n",
    "    'OW2': 'OW1',\n",
    "    'OY2': 'OY1',\n",
    "    'UH2': 'UH1',\n",
    "    'UW2': 'UW1',\n",
    "}\n",
    "\n",
    "with open('cmudict.dict') as f:\n",
    "    for l in f:\n",
    "        l = re.sub(r'#.*', '', l.strip())\n",
    "        parts = l.split()\n",
    "        word = parts[0]\n",
    "        word = re.sub(r'\\(.*\\)', '', word)\n",
    "        phones = parts[1:]\n",
    "        phones = [phone_remapping[ph] if ph in phone_remapping else ph for ph in phones]\n",
    "        assert all(ph in all_phones for ph in phones) \n",
    "        en_g2p_dict[word].append(phones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "osMsezj3YJHW"
   },
   "outputs": [],
   "source": [
    "# Here you can add custom words\n",
    "custom_dict = {\n",
    "    'waveglow': 'W EY1 V G L OW0'.split(),\n",
    "    'spartaaa': 'S P AH1 R T AH1 AH1 AH1'.split()\n",
    "}\n",
    "for pron in custom_dict.values():\n",
    "    for ph in pron:\n",
    "        assert ph in phone2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7HHK2bhcYJHb"
   },
   "outputs": [],
   "source": [
    "# Preprocessor: Text -> phone indexes\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "def preprocess(sent):\n",
    "    words = tokenizer.tokenize(sent.lower())\n",
    "    phones = []\n",
    "    for word in words:\n",
    "        if re.fullmatch(r'[^a-z]*', word):\n",
    "            phones += ['pau']\n",
    "        elif word in custom_dict:\n",
    "            phones += custom_dict.get(word)\n",
    "        elif word in en_g2p_dict:\n",
    "            phones += en_g2p_dict.get(word)[0]\n",
    "        else:\n",
    "            raise ValueError('No transcription for word \"{}\"\"'.format(word))\n",
    "    phone_idxs = [phone2idx[phone] for phone in phones]\n",
    "    return phone_idxs\n",
    "\n",
    "preprocess('My waveglow!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0c-cA1VRxuim"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VmQfQ6TnYJHf"
   },
   "outputs": [],
   "source": [
    "## Finally synthesize!\n",
    "\n",
    "text = 'This. . . is. . .  SPARTAAA!!!!!'\n",
    "mels = model.forward_inference(np.array(preprocess(text)))\n",
    "mels = mels.numpy()[0].T\n",
    "synthesize(mels)\n",
    "\n",
    "plt.imshow(mels[::-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SFp8bxDtwuud"
   },
   "source": [
    "## Alternative vocoder\n",
    "\n",
    "Below we implement a simple algorithmic vocoder from pre-wavenet era\n",
    "Feel free to try it and see find out which works best for you :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdRsTw3WxXa9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UCd_8qwsYJHk"
   },
   "outputs": [],
   "source": [
    "import scipy.signal as dsp\n",
    "import librosa.filters as filters\n",
    "import scipy.interpolate as interp\n",
    "mel_filters = filters.mel(22050, 1024, fmin=0, fmax=8000, n_mels=NMels)\n",
    "mel_inv = np.linalg.pinv(mel_filters)\n",
    "\n",
    "def robot_synth(mels, speed=1., spectr_deform=1.):\n",
    "    assert mels.shape[0] == 80 and mels.ndim == 2\n",
    "    spectr = np.exp(mels).T @ mel_inv.T\n",
    "    phase = np.random.uniform(0, 1, size=[1,513])\n",
    "    spectr = spectr * np.exp(2j * np.pi * phase)\n",
    "    _, waveform = dsp.istft(spectr.T, nperseg=1024, noverlap=int(1024 - (256 / speed)))\n",
    "    ipd.display(ipd.Audio(data=waveform, rate=22050, autoplay=True))\n",
    "    \n",
    "\n",
    "robot_synth(mels[0].T, speed=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPP3uu-wx3x2"
   },
   "source": [
    "### Homework assignment & grades\n",
    "\n",
    " Implement & train the base model that it technically works (prints batches, etc)\n",
    "\n",
    "\n",
    "__(4 points)__ Train a bigger & better model. Try to enhance the architecture using your nlp expertise :) If you're out of creative ideas, at least make sure that encoder uses **bidirectional** recurrent network. Your model should be to at least such a performance where you (and we) can decipher what it says.\n",
    "\n",
    "\n",
    "__(3+ points)__ Choose one (or more) of possible ways to enhance speech synthesis:\n",
    " * __Batch sorting:__ your model can process sequences more effeciently if you pack them right. The core idea is to avoid excessive __PAD__-ding by forming batches of approximately equal sequence length.\n",
    "    * Simple heuristic: sort all sequences by their length plus small random noise, then split into chunks\n",
    "    * shuffle chunks and feed them as minibatches. That way all batches will have adjacent length.\n",
    "    * You can futher improve performance by adapting batch size to sequence length. Shorter sequences = larger batch.\n",
    "    \n",
    " * __Scheduled sampling:__ when training, your model always receives reference mels as previous inputs. However, during inference the model is fed with its own previous outputs that are different from references.\n",
    "    * Hence, if your model makes an error, this error is likely to propagate to further time-steps.\n",
    "    * One way to fix that: during training, randomly choose a subset of time-steps where model gets its own output as input\n",
    "    * This technique is known as __scheduled samplng__, you can read more about it in the [original paper](https://arxiv.org/abs/1506.03099).\n",
    "    \n",
    " * __Post-net:__ while your recurrent network is able to synthecise reasonable spectrograms, you can further improve it using references by training additional model that __post-processes the fully generated spectrogram__.\n",
    "    * In speech synthesis, this model is known as __Post-net__, it is usually a convolutuional neural network\n",
    "    * an important trick here is that you train both your recurrent neural network and your post-net to by minimizing a sum of two mel losses\n",
    " \n",
    " \n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "APkMZam0x6fF"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "seminar_tts.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
