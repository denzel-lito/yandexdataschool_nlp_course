#### Week 10: Presentations on NMT (14th November).

* Form groups of 4 or 5 students.
* Pick a topic.
* Read the papers.
* Prepare a 10-minute presentation (3 slides).

Presentations will be given in class on 14th Nov. 
 
Please sign up to the [spreadsheet](https://docs.google.com/spreadsheets/d/1mqqZpngL81q8x6YXz4NhDSYbVh2W5rIw32sfPeTpW1A/edit?usp=sharing) using your login for Anytask. One group per line.  

Each person should read at least one paper and your group should meet in advance of the class to finalize your presentation. 

As well as explaining the main ideas in the papers, please also pay attention to any problems with the experimental set up in the papers and comment on whether their conclusions are well supported by their results.

1. Computational bottlenecks in NMT
  * [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/pdf/1508.07909.pdf)
  * [Simple, Fast Noise-Contrastive Estimation for Large RNN Vocabularies](http://www.aclweb.org/anthology/N16-1145.pdf)
  * [Vocabulary Manipulation for Neural Machine Translation](http://www.aclweb.org/anthology/P16-2021)
  * [Fully Character-Level Neural Machine Translation without Explicit Segmentation](http://aclweb.org/anthology/Q17-1026)

2. Attention for NMT
  * [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
  * [Look Harder: A Neural Machine Translation Model with Hard Attention](https://www.aclweb.org/anthology/P19-1290/)
  * [Fine-grained attention mechanism for neural machine translation](https://arxiv.org/abs/1803.11407)
  * [What does Attention in Neural Machine Translation Pay Attention to?](https://www.aclweb.org/anthology/I17-1004.pdf)

3. Transformers vs. RNNs
  * [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
  * [Combining Recent Advances in Neural Machine Translation](https://arxiv.org/pdf/1804.09849.pdf)
  * [How Much Attention Do You Need?](https://www.aclweb.org/anthology/P18-1167.pdf)  
  * [The Importance of Being Recurrent for Modeling Hierarchical Structure](https://arxiv.org/pdf/1803.03585.pdf)

4. Analyzing Transformers
  * [Learning Deep Transformer Models for Machine Translation](https://arxiv.org/abs/1906.01787)
  * [Analyzing Multi-Head Self-Attention](https://arxiv.org/pdf/1905.09418.pdf) 
  * [Understanding and Improving Transformer From a Multi-Particle Dynamic System Point of View](https://arxiv.org/abs/1906.02762)
  * [Transformer Disection](https://arxiv.org/pdf/1908.11775.pdf)

5. Using Monolingual Data to Improve NMT
  * [On Using Monolingual Corpora in Neural Machine Translation](https://arxiv.org/pdf/1503.03535.pdf)
  * [Improving Neural Machine Translation Models with Monolingual Data](https://arxiv.org/abs/1511.06709)
  * [Understanding Back-Translation at Scale](https://arxiv.org/pdf/1808.09381.pdf)
  * [Back-Translation Sampling by Targeting Difficult Words in Neural Machine Translation](https://arxiv.org/pdf/1808.09006.pdf)
  * [On The Evaluation of Machine Translation Systems Trained With Back-Translation](https://arxiv.org/pdf/1908.05204.pdf)

6. Low Resource NMT
  * [Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism](http://www.aclweb.org/anthology/N16-1101)
  * [Contextual Parameter Generation for Universal Neural Machine Translation](https://arxiv.org/abs/1808.08493)
  * [Dual Learning for Machine Translation](https://arxiv.org/pdf/1611.00179.pdf)
  * [Zero-Shot Dual Machine Translation](https://arxiv.org/abs/1805.10338)
  * [Phrase-Based & Neural Unsupervised Machine Translation](https://arxiv.org/pdf/1804.07755.pdf)
  * [Revisiting Low-Resource Neural Machine Translation: A Case Study](https://arxiv.org/abs/1905.11901)
  * [Massively Multilingual Neural Machine Translation
in the Wild](https://arxiv.org/pdf/1907.05019.pdf)
  * [Towards Making the Most of BERT in Neural Machine Translation](https://arxiv.org/abs/1908.05672)

7. Noisy Channel NMT
  * [Neural Machine Translation with Reconstruction](https://arxiv.org/abs/1611.01874)
  * [The Neural Noisy Channel](https://arxiv.org/abs/1611.02554)
  * [Correcting Length Bias in Neural Machine Translation](http://statmt.org/wmt18/pdf/WMT022.pdf)
  * [Simple and Effective Noisy Channel Modeling for Neural Machine Translation](https://arxiv.org/abs/1908.05731)
  * [Putting Machine Translation in Context with the Noisy Channel Model](https://arxiv.org/abs/1910.00553)
 

8. Alternative decoding strategies
  * [Non-Autoregressive Neural Machine Translation](https://arxiv.org/pdf/1711.02281.pdf)
  * [Latent-Variable Non-Autoregressive Neural Machine Translation](https://arxiv.org/pdf/1908.07181.pdf)
  * [Insertion Transformer: Flexible Sequence Generation via Insertion Operations](https://arxiv.org/abs/1902.03249)
  * [Insertion-based Decoding with automatically Inferred Generation Order](https://arxiv.org/abs/1902.01370)
  * [Efficient Bidirectional Neural Machine Translation](https://arxiv.org/pdf/1908.09329.pdf)
  * [Dynamic Past and Future for Neural Machine Translation](https://arxiv.org/pdf/1904.09646.pdf)

9. Edit-Based Models
  * [Search Engine Guided Non-Parametric Neural Machine Translation](https://arxiv.org/abs/1705.07267)	
  * [Guiding Neural Machine Translation with Retrieved Translation Pieces](https://arxiv.org/pdf/1804.02559)
  * [Generating Sentences by Editing Prototypes](https://arxiv.org/abs/1709.08878)
  * [Levenshtein Transformer](https://arxiv.org/abs/1905.11006)

10. Has NMT really bridged the gap between MT and human translation? What problems remain?
  * [Achieving Human Parity on Automatic Chinese to English News Translation](https://www.microsoft.com/en-us/research/uploads/prod/2018/03/final-achieving-human.pdf)
  * [Six Challenges for NMT](http://www.aclweb.org/anthology/W17-3204)
  * [Analyzing Uncertainty in Neural Machine Translation](https://arxiv.org/pdf/1803.00047.pdf)
  * [Towards Robust Neural Machine Translation](https://arxiv.org/pdf/1805.06130.pdf)
  * [Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation](https://www.aclweb.org/anthology/D18-1512/)
