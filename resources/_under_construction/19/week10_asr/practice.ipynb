{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice: Voice Command Recognition \n",
    "\n",
    "![img](https://yastatic.net/s3/lpc/bf4f0ad7-739e-4e52-a370-ec173244aa6c.png)\n",
    "\n",
    "Today you're finally gonna deal with speech! We'll walk you through all the main steps of speech processing pipeline and you'll get to write your own voice recognition system. It's gonna be fun! _(they said)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "!wget https://github.com/yandexdataschool/nlp_course/raw/2019/week10_asr/sample1.wav -O sample1.wav\n",
    "!wget https://github.com/yandexdataschool/nlp_course/raw/2019/week10_asr/sample2.wav -O sample2.wav\n",
    "display(Audio(\"sample1.wav\"))\n",
    "display(Audio(\"sample2.wav\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider an audio sample above. The reason you can hear a sound is because your speakers/headphones cause oscillations of air that reach your ears. There's a part of your inner ear called [Cochlea](https://en.wikipedia.org/wiki/Cochlea) that looks like a spiral where every spot is receptive to a specific range of sound frequency.\n",
    "![img](https://i.pinimg.com/originals/f7/ed/7e/f7ed7eeb0c4e45b9a35dff9465805b48.jpg)\n",
    "\n",
    "The sound itself can be recorded as a collection of __amplitudes__ of these oscillations over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitudes, sample_rate = librosa.core.load(\"./sample1.wav\")\n",
    "\n",
    "print(\"Length: {} seconds at sample rate {}\".format(amplitudes.shape[0] / sample_rate, sample_rate))\n",
    "plt.figure(figsize=[16, 4])\n",
    "plt.title(\"First 10^4 out of {} amplitudes\".format(len(amplitudes)))\n",
    "plt.plot(amplitudes[:10000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Mel-Spectrogram (3 points)\n",
    "\n",
    "As you can see, amplitudes follow a periodic patterns with different frequencies. However, it is very difficult to process these amplitudes directly because there's so many of them! A typical WAV file contains 22050 amplitudes per second, which is already way above a typical sequence length for other NLP applications. Hence, we need to compress this information to something manageable.\n",
    "\n",
    "A typical solution is to use __spectrogram:__ instead of saving thousands of amplitudes, we can perform Fourier transformation to find which periodics are prevalent at each point in time. More formally, a spectrogram applies [Short-Time Fourier Transform (STFT)](https://en.wikipedia.org/wiki/Short-time_Fourier_transform) to small overlapping windows of the amplitude time-series:\n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Phillip_Lobel/publication/267827408/figure/fig2/AS:295457826852866@1447454043380/Spectrograms-and-Oscillograms-This-is-an-oscillogram-and-spectrogram-of-the-boatwhistle.png\" width=\"480px\">\n",
    "\n",
    "However, this spectrogram may have extraordinarily large numbers that can break down neural networks. Therefore the standard approach is to convert spectrogram into a __mel-spectrogram__ by changing frequencies to [Mel-frequency spectrum(https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)].\n",
    "\n",
    "Hence, the algorithm to compute spectrogram of amplitudes $y$ becomes:\n",
    "1. Compute Short-Time Fourier Transform (STFT): apply fourier transform to overlapping windows\n",
    "2. Build a spectrogram: $S_{ij} = abs(STFT(y)_{ij}^2)$\n",
    "3. Convert spectrogram to a Mel basis\n",
    "\n",
    "\n",
    "By far the hardest pard of this is computing STFT, so let's focus on that first:\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some helpers:\n",
    "# 1. slice time-series into overlapping windows\n",
    "def slice_into_frames(amplitudes, window_length, hop_length):\n",
    "    return librosa.core.spectrum.util.frame(\n",
    "        np.pad(amplitudes, int(window_length // 2), mode='reflect'),\n",
    "        frame_length=window_length, hop_length=hop_length)\n",
    "    # output shape: [window_length, num_windows]\n",
    "\n",
    "dummy_amps = amplitudes[2048: 6144]\n",
    "dummy_frames = slice_into_frames(dummy_amps, 2048, 512)\n",
    "\n",
    "plt.figure(figsize=[16, 4])\n",
    "plt.subplot(121, title='Whole audio sequence', ylim=[-3, 3])\n",
    "plt.plot(dummy_amps)\n",
    "\n",
    "plt.subplot(122, title='Overlapping frames', yticks=[])\n",
    "for i, frame in enumerate(dummy_frames.T):\n",
    "    plt.plot(frame + 10 - i);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Weights for window transform. Before performing FFT you can scale amplitudes by a set of weights\n",
    "# The weights we're gonna use are large in the middle of the window and small on the sides\n",
    "dummy_window_length = 3000\n",
    "dummy_weights_window = librosa.core.spectrum.get_window('hann', dummy_window_length, fftbins=True)\n",
    "plt.plot(dummy_weights_window); plt.plot([1500, 1500], [0, 1.1], label='window center'); plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fast Fourier Transform in Numpy. Note: this function can process several inputs at once (mind the axis!)\n",
    "dummy_fft = np.fft.rfft(dummy_amps[:3000, None] * dummy_weights_window[:, None], axis=0)  # complex[sequence_length, num_sequences]\n",
    "plt.plot(np.real(dummy_fft)[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now it's time to combine everything into a __S__hort-__T__ime __F__ourier __T__ransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_STFT(amplitudes, window_length, hop_length):\n",
    "    \"\"\" Compute short-time Fourier Transform \"\"\"\n",
    "    # slice amplitudes into overlapping frames [window_length, num_frames]\n",
    "    frames = <YOUR CODE>\n",
    "    \n",
    "    # get weights for fourier transform, float[window_length]\n",
    "    fft_weights = <YOUR CODE>\n",
    "    \n",
    "    # apply fourier transfrorm to frames scaled by weights\n",
    "    stft = <YOUR CODE>\n",
    "    return stft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_melspectrogram(amplitudes, sample_rate=22050, n_mels=128,\n",
    "                       window_length=2048, hop_length=512, fmin=1, fmax=8192):\n",
    "    \"\"\"\n",
    "    Implement mel-spectrogram as described above.\n",
    "    :param amplitudes: float [num_amplitudes], time-series of sound amplitude, same as above\n",
    "    :param sample rate: num amplitudes per second\n",
    "    :param n_mels: spectrogram channels\n",
    "    :param window_length: length of a patch to which you apply FFT\n",
    "    :param hop_length: interval between consecutive windows\n",
    "    :param f_min: minimal frequency\n",
    "    :param f_max: maximal frequency\n",
    "    :returns: mel-spetrogram [n_mels, duration]\n",
    "    \"\"\"\n",
    "    # Step I: compute Short-Time Fourier Transform\n",
    "    stft = <YOUR CODE>\n",
    "    assert stft.shape == (window_length // 2 + 1, len(amplitudes) // 512 + 1)\n",
    "    \n",
    "    # Step II: convert stft to a spectrogram\n",
    "    spectrogram = <YOUR CODE>\n",
    "    \n",
    "    # Step III: convert spectrogram into Mel basis (multiplying by transformation matrix)\n",
    "    mel_basis = librosa.filters.mel(sample_rate, n_fft=window_length,\n",
    "                                    n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
    "    # ^-- matrix [n_mels, window_length / 2 + 1]\n",
    "    \n",
    "    mel_spectrogram = <TOUR CODE>\n",
    "    assert mel_spectrogram.shape == (n_mels, len(amplitudes) // 512 + 1)\n",
    "    \n",
    "    return mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitudes1, _ = librosa.core.load(\"./sample1.wav\")\n",
    "amplitudes2, _ = librosa.core.load(\"./sample2.wav\")\n",
    "ref1 = librosa.feature.melspectrogram(amplitudes1, sr=sample_rate, n_mels=128, fmin=1, fmax=8192)\n",
    "ref2 = librosa.feature.melspectrogram(amplitudes2, sr=sample_rate, n_mels=128, fmin=1, fmax=8192)\n",
    "assert np.allclose(get_melspectrogram(amplitudes1), ref1, rtol=1e-4, atol=1e-4)\n",
    "assert np.allclose(get_melspectrogram(amplitudes2), ref2, rtol=1e-4, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"That's no moon - it's a space station!\"); plt.xlabel(\"Time\"); plt.ylabel(\"Frequency\")\n",
    "plt.imshow(get_melspectrogram(amplitudes1))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Help me, Obi Wan Kenobi. You're my only hope.\"); plt.xlabel(\"Time\"); plt.ylabel(\"Frequency\")\n",
    "plt.imshow(get_melspectrogram(amplitudes2));\n",
    "\n",
    "# note that the second spectrogram has higher mean frequency corresponding to the difference in gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech Commands Dataset\n",
    "\n",
    "We're now going to train a classifier to recognize voice. More specifically, we'll use the [Speech Commands Dataset]  that contains around 30 different words with a few thousand voice records each. Naturally, we'll preprocess each training sample using the `get_melspectrogram` you designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "datadir = \"speech_commands\"\n",
    "\n",
    "!wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz\n",
    "# alternative url: https://www.dropbox.com/s/j95n278g48bcbta/speech_commands_v0.01.tar.gz?dl=1\n",
    "!mkdir {datadir} && tar -C {datadir} -xvzf speech_commands_v0.01.tar.gz 1> log\n",
    "\n",
    "samples_by_target = {\n",
    "    cls: [os.path.join(datadir, cls, name) for name in os.listdir(\"./speech_commands/{}\".format(cls))]\n",
    "    for cls in os.listdir(datadir)\n",
    "    if os.path.isdir(os.path.join(datadir, cls))\n",
    "}\n",
    "print('Classes:', ', '.join(sorted(samples_by_target.keys())[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by training a model to recognize direcions. Imagine a mobile robot that you can control via speech. You say \"Go right\" and it obeys. Or it doesn't - depending on how you trained it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "from tqdm import tqdm\n",
    "import joblib as jl\n",
    "\n",
    "classes = (\"left\", \"right\", \"up\", \"down\", \"stop\")\n",
    "\n",
    "def preprocess_sample(filepath, max_length=150):\n",
    "    amplitudes, sr = librosa.core.load(filepath)\n",
    "    spectrogram = get_melspectrogram(amplitudes, sample_rate=sr)[:, :max_length]\n",
    "    spectrogram = np.pad(spectrogram, [[0, 0], [0, max(0, max_length - spectrogram.shape[1])]], mode='constant')\n",
    "    target = classes.index(filepath.split(os.sep)[-2])\n",
    "    return np.float32(spectrogram), np.int64(target)\n",
    "\n",
    "all_files = chain(*(samples_by_target[cls] for cls in classes))\n",
    "spectrograms_and_targets = jl.Parallel(n_jobs=-1)(tqdm(list(map(jl.delayed(preprocess_sample), all_files))))\n",
    "X, y = map(np.stack, zip(*spectrograms_and_targets))\n",
    "X = X.transpose([0, 2, 1])  # to [batch, time, channels]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train a model: (3 points)__ finally, lets' build and train a classifier neural network. You can use _any_ library you like. If in doubt, consult the model & training tips below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras, keras.layers as L\n",
    "\n",
    "# Build, compile and train a model. If you're out of ideas, see hints below\n",
    "# Use any framework you want, the stub below is optional\n",
    "model = keras.models.Sequential([\n",
    "    L.InputLayer(input_shape=(None, 128)),  # (batch), time, channels\n",
    "    <YOUR CODE>\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "<YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.mean(model.predict(X_val).argmax(-1) == y_val)\n",
    "print(\"Final Accuracy:\", accuracy)\n",
    "assert accuracy >= 0.85, \"Your model is not good enough. Yet.\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Training tips:__ here's what you can try:\n",
    "* __Layers:__ 1d or 2d convolutions, perhaps with some batch normalization in between;\n",
    "* __Architecture:__ VGG-like, residual, highway, densely-connected - you name it :)\n",
    "* __Batch size matters:__ smaller batches usually train slower but better. Try to find the one that suits you best.\n",
    "* __Data augmentation:__ add background noise, faster/slower, change pitch;\n",
    "* __Average checkpoints:__ you can make model more stable with [this simple technique (arxiv)](https://arxiv.org/abs/1803.05407)\n",
    "* __For full scale stage:__ make sure you're not losing too much data due to max_length in the pre-processing stage!\n",
    "\n",
    "These are just recommendations. As long as your model works, you're not required to follow them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main quest: full scale recognition (4+ points)\n",
    "\n",
    "Your final task is to train a full-scale voice command spotter and apply it to a video:\n",
    "1. Build the dataset with all 30+ classes (directions, digits, names, etc.)\n",
    "  * __Optional:__ include a special \"noise\" class that contains random unrelated sounds\n",
    "  * You can download youtube videos with [`youtube-dl`](https://ytdl-org.github.io/youtube-dl/index.html) library.\n",
    "2. Train a model on this full dataset. Kudos for tuning its accuracy :)\n",
    "3. Apply it to a audio/video of your choice to spot the occurences of each keyword\n",
    " * Here's one [video about primes](https://www.youtube.com/watch?v=EK32jo7i5LQ) that you can try. It should be full of numbers :)\n",
    " * There are multiple ways you can analyze the performance of your network, e.g. plot probabilities predicted for every time-step. Chances are you'll discover something useful about how to improve your model :)\n",
    "\n",
    "\n",
    "As usual, please briefly describe what you did in a short informal report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<A WHOLE LOT OF YOUR CODE>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
